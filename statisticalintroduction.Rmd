---
title: "Statistic Manual"
subtitle: Created by Mai-Britt Berghöfer
date: "1 January 2023"
output:
  html_document:
    theme: journal
    toc: yes
    toc_float: true
  pdf_document:
    keep_tex: yes
    latex_engine: xelatex
  html_notebook: 
    theme: journal
    toc: true
    toc_float: true
fontsize: 12pt
---

# Hands on: Statistical Introduction to a low flow analysis 


## How to follow this Guide 

This guide is supposed to an interactive guide to learn the statistical approach of low flow analysis. The best way to learn how to analyze discharge measurements or other time series yourself is to follow this guide interactively by running the code yourself. The entire manual will be accompanied and based on the "R language and environment for statistical computing" [R Core Team, 2022]. It is an open source software and freely accessible (https://www.r-project.org/about.html).

The **lfanalyse** R-package was developed in the framework of this project and combines functions to analyze a discharge time series using descriptive statitics, threshold based low flow analysis and by estimating trends. For further information, please visit the package repository and find the description of the package: https://github.com/maibrittbergh/lfanalyse. 

To follow this guide it will be useful to install the package to your R:
```{r}

# Please install the devtools package in case it's not already in your library.
#install.packages("devtools") 
library(devtools)
install_github("maibrittbergh/lfanalyse")
library(lfanalyse)
```


### Optional Dataset

The  shiny [Chang et al., 2021] WebApp linked to this manual dealt with the GRDC-Dataset [GRDC, 2023]. Further information about this Project are available under the github repository of the webApp: https://github.com/maibrittbergh/lfwebapp. 

The GRDC data set is continuously updated by the Global Runoff Data Centre (GRDC) and freely available under the following link: https://www.bafg.de/GRDC/EN/02_srvcs/21_tmsrs/210_prtl/prtl_node.html [GRDC, 2021]. It contains averaged daily or monthly discharge values for more than 10,000 stations in 159 countries [BFG, 2020]. The values unit is cubic metres per second. 
To follow this guide for a specific station of interest please download it online. Otherwise, feel free to proceed following this guide by likewise using the example station "Dresden" in eastern Germany for a statistical analysis. This dataset can be found in the repository of this manual (https://github.com/maibrittbergh/statisticalmanual) and was downloaded from the GRDC data center in June 2020.


A comment on the format: In case you downloaded data from the GRDC Website [GRDC, 2023] you can easily load the dataset using the lfanalyse function **grdc_reader**. This function reads in all the locally saved GRDC datasets and stores them into a list. Each entry of the list is a different station. Furthermore you can use the lfanalyse function **metadata_grdc** to receive a metadatatable that offers you a first insight of all the stations you selected online. The example data (DRESDEN) is arranged in a list as well to maintain consistency within the project.)

To work with the example dataset: Please downlad and save the file: data.rds. 
```{r}
#install.packages("readr")
library(readr) #package is useful to read rds files.
#data=read_rds("path to the local folder, where you saved the rds file")

```

Now you will see a list in your working directory.

## Section 1: The goal of the Analysis

The goal of this analysis is to detect whether one can observe a trend of observed discharge data. Ahead of the analysis it is important to discuss 

In this Analysis a linear model was choosen to express a trend #expected a linear corr between time and discharge #Source 

## Section 2: Descriptive Statistics: The Variety of the Dataset

Although there is not one model that fits best to all dataset, we decided to stick to the linear model to be able to compare the outputs with each other. Of course we are very likely to loose information by trying to summarize the entire dataset in straigh lines of different slopes. For example: It could be possible that due to an increasing number of extreme heat events since 2000 there might have also more water scarcity events since 2000. But if we only adjust a straight line from 1860 to 2000 we might loose information. Other models? This is an important and right point that leads us directly to a whole debate. All around the sentence: 

To put it in George E. P. Box's words: “All models are wrong, but some are useful”

#Overfitting debate 

*Descriptive statistics* is the first step to many statistical analyses. It helps us to get a general idea of the dataset. How long is our time series? Maybe we want to know average values, different quantiles to be able to make a statment about low flow events? Looking at general properties of the timeseries, we might also be able to critically question our data. Sticking to the example of discharge data: maybe construction measures lead to an abrupt change of our average discharge value? Perhaps other structural interventions in the catchment area lead to a large disruption in our time series. Besides only looking at the **mean** and the **standard deviation** we should consider to read further about a station of our interest to be able to make general assumptions about the factors the lead to a change in discharge measurements. One need to have in mind that this claim could not be implemented in the Webapp. 




## Section 3: Prediction or statistical Inference?

The overall assumption that there is any kind of relation between the discharge Value (Y) and the Time (X) we can write in a general form: 

y= f(X)+ε (3.1)

Independently of the example to analyze discharge data, f(X) is not known but fixed and ε is an error term, independetn of X and has zero as a mean. From f we learn what x can tell us about y. At this point it should be mentioned that f could be constructed of more than one input variable. In many cases estimating f is very important, like: 

1. For Prediction

One can easily think of a situation where the input (x) is available but one can't make any assumptions concerning y. Therefor we need to make a prediction $\hat{y}$ of y finding $\hat{f}$ that provides the most accuracy for y. In this case one can treat $\hat{f}$  like a **black box**. Therefor $\hat{f}$  will not be the perfect guess for our function f(x). How good our estimations of $\hat{y}$ for y are depends on the **reducible** and the **irreducible** error. The closer  $\hat{f}$ is to f the more we can reduce this **reducible** error. Nevertheless y is a function of ε (1.1) therefore it is not possible to only predict it with x (or f(x)). This gap is described as the irreducible error. 
For a prediction  one can set ε=0 as it may contain unmeasured quantities  that might hold additional information for the prediction, but as they are not measured there is no way to conlude them into our prediction. Therefore the prediction term is: 

$\hat{y}=\hat{f}(x)$ (3.2)


2. For Inference

Oftentimes we want to assess the relation between y and x ($x_1, x_2,..., x_n$). In this case it is our goal to find out what f is although we dont want to make predictions for y. 

Of course there is modeling consisting of prediction and inference at the same time as well. Depending on our goal or problem we can think of different ways to specify f. A linear model allows assumptions of statistical inference on first sight but might not always be appropriate for predictions. 


## Section 4: The Linear model



As mentioned above (Section 3) $\hat{y}(x)$ clarifies that we speak about a prediction of y (dependent on x). It is important to keep in mind that the predictors/ parameters are unknown and need to estimated in a way that the resulting model fits the data as good as possible. **As good as possible** can also be expresses numerically by determining the error:

$e_i=y_i-\hat{y}_i$. (4.3)

e represents the ith residual (distance between the ith response variable and predicted variable). To avoid the problem that negativ residuals cancel out the positive residuals the **RSS: residual sum of sqaures is defined**: 

$RSS=e_1^2+e_2^2+...e_n^2$ (4.4)

By minimizing the **RSS** we obtain the best $w_0$ and $w_1$, this is described be the **Linear Least Squares Approach**:

$w_1=\frac{\sum_{i=1}^{n} (x_i-\overline{x})(y_i-\overline{y}}{\sum_{i=1}^{n} (x_i-\overline{x})^2}$ (5)

$w_0=\overline{y}-w_1\overline{x}$ (4.5)



From our assumptions (Section 1) we were assuming that there is a relationship between x and y: $y= f(X)+ε$. Furthermore we believe that the data and therefore f(x) can be represented  by a linear function. The term "linear" refers to the linear model parameters/ coefficients. 

A linear model is constructed from a linear function: 
$y = w_0 + w_1* x$ (4.1)


To build a model from the linear function (4.1) we construct the linear model by adding noise: 

$y=w_0+w_1*x+ε$ (4.2)

In this case: 

1. $w_0, w_1$ are our cofficents/ parameters: $w_0$ is the intercept and $w_1$ the slope of our model
2. $x$ is our predictor 
3. $ε$ the error term includes everything that we can't adress with this simple equation ($ε$ is normally treated to be independent of x) and follows a certain distribution.

Disclaimer: Equation (4.2) doesn't describe the population regression line (which we usually dont have for measured/observed data) but the *least squares line*. The difference between both lines is our approach to use information generated by a sample to make assumptions about a large population. While the populare regression line is our **true** line, the least squares approach only refers to a sample of the large population.
There is only one way to deal with this problem: How do we guarantee that our observations are unbiased? Taking more samples: The average of a large number of sample means $\hat{µ}$ can be regarded as the real mean µ. Our µ is **unbiased**. If we would only consider one sample mean $\hat{µ}$ to estimate µ this approach could be biased as this value could possibly over- or underestimate the real mean µ. 

We can also formulate in a number how exact our $\hat{µ}$ functions as an estimate for µ by introducing the **standard error**:

$Var(\hat{µ})=SE(\hat{µ})^2=\frac{σ^2}{n}$ (4.3)

This quantity tells us the average distance from $\hat{µ}$ to µ and how it decreases with n. What do we learn from that regarding our "simple" linear regression? The estimates of $w_0$ and $w_1$ are better and our standard error of these quantities is smaller the bigger our dataset is.
Assmuning that the errors of all the observations are uncorrelated but have a common variance $σ^2$ we can say: 

$SE(\hat{w_0})^2=σ^2[{\frac{1}{n}+\frac{\overline{x}^2}{\sum_{i=1}^{n} (x_i-\overline{x})}}]$

$SE(\hat{w_1})^2=\frac{σ^2}{\sum_{i=1}^{n} (x_i-\overline{x})}$

One should formally write  $\hat{w_0}$ and $\hat{w_1}$ if $σ^2$ following our thoughts from Section 3. In order to keep the script simple we stick to ${w_0}, {w_1}$.



## Section 5: The Noise

Everything that is not explained with our model? This could just be a measurement error that we can never really rule out. Also its possible that there are other variables than x that cause y to vary. Therefore looking at the noise can be very interesting to detect underlying trends and factors [A. King, 2018].

To understand noise better we will have a look at the words of Parzen’s (1960) about randomness: "

A random (orchance) phenomenon is an empirical phenomenon characterized by the property that its observation under a given set of circumstances does not always lead to the same observed outcomes (so that there is no deterministic regularity) but rather to different outcomes in such a way that there is statistical regularity. By this is meant that numbers exist between 0 and 1 that represent the relative frequency with which the different possible outcomes may be observed in a series of observations of independent occurrences of the phenomenon..."

In many natural sciences our data is neither excact nor reproducible. In addition there is an infinte number of parameters in the earth system so that our models will always be approximations [Scales and Snider, 1998].

In our example, looking at discharge data,it's probably not necessary to mention that precipitation, temperature, ground water storage.. as well have an influence on our variable. Although it might seam obvious it is important to be mentioned as these variables partly explain uncertainties in our approach and results. 

Even if we would be able to model every measurable influencing variable exactly our model would necessarily gain in significance. With a large number of degrees of freedom one can fit nay data - but is it worth fitting?[Scales and Snider, 1998].

After Dobrin and Savit (1988) "noise that part of the data that we choose not to explain".

As we are not able to reproduce exactly this outcome we might as well reduce the random noise by averaging [Scales and Snider, 1998].

The noise follows a certain distribution which could be gaussian, laplace, Student's t...

In our analysis we assumed the noise to be gaussian distributed. This means that our predicted value for any value of x is:

$\hat{y_i}=µ_i= w_0 + w_1* x$ (5.1)

Assuming a gaussian normal distribution for the noise, we say that the data values are normaly distributed around the mean $\hat{µ}$ [Kruschke, 2015]. Our linear least squares regression line gives the mean of these distributions at any point. 


### How many Predictors?
We distinguish between Univariate models (with just a single predictor) and Multivariate models with more than one predictor. As soon as we have multiple predictors it is more complicated to interpreatate the coefficients as they are dependent on the other variables in the model [Data Analysis Andrew Gelman]. 

In our analysis we will only look at a univariate model as our data   [GRDC, 2021] simply consits of daily discharge measurements.







# Sources: 

- [Data Analysis Andrew Gelman ]
- Korup 
- LSI : Learning stat_ introduction
-  R Core Team (2022). R: A language and environment for statistical
R Core Team 2022: computing. R Foundation for Statistical Computing, Vienna, Austria.
 URL https://www.R-project.org/.

[GRDC, 2023]
© 2018 Aaron A. King.: https://kinglab.eeb.lsa.umich.edu/480/prob/prob.html
[Chang et al., 2021]- shiny package


- -                GEOPHYSICS, VOL. 63, NO. 4 (JULY-AUGUST 1998); P. 1122–1124, 2 FIGS.What is noise?John A. Scales∗and Roel Snieder



[Kruschke, 2015]- Doing Baysian Data Analysis